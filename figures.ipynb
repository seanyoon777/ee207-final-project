{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1. Example data diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Baselines.KalmanDecoder import * \n",
    "from SNNModel.dataloader import *\n",
    "from Globals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from neurobench.datasets import PrimateReaching\n",
    "\n",
    "data_path = './data/indy_20160630_01.mat'\n",
    "\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, spike_units='all', normalize=True, center_zero=True, aggreagate_chs=True, bin_size=1, bad_ch_cutoff=1):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            spike_units: string of list of spike unit numbers to use (e.g. [0, 2, 4])\n",
    "            normalize (bool, optional): _description_. Defaults to True.\n",
    "            bin_size (int, optional): _description_. Defaults to 1.\n",
    "            ex_cutoff : excludes spiking units that spike less than this cutoff value \n",
    "        \"\"\"\n",
    "        self.pr_dataset = PrimateReaching(file_path='data',\n",
    "                                          filename=\"indy_20160630_01.mat\",\n",
    "                                          num_steps=1,\n",
    "                                          train_ratio=0.5,\n",
    "                                          bin_width=0.004,\n",
    "                                          biological_delay=0,\n",
    "                                          download=False)\n",
    "        print(\"Loaded pr dataset\")\n",
    "        \n",
    "        self.spike_units   = spike_units \n",
    "        self.normalize     = normalize \n",
    "        self.center_zero   = center_zero \n",
    "        self.aggregate_chs = aggreagate_chs\n",
    "        self.bin_size      = bin_size\n",
    "        self.bad_ch_cutoff = bad_ch_cutoff\n",
    "        \n",
    "        with h5py.File(data_path, 'r') as data:\n",
    "            # initialize spike units to use \n",
    "            if spike_units == 'all': \n",
    "                n_sp = data['spikes'].shape[0]\n",
    "                self.spike_units = [i for i in range(n_sp)]\n",
    "            \n",
    "            # get spike matrix and split into train / test set \n",
    "            spike_matrix = self._get_spike_mat(data)\n",
    "            trainX, testX, train_finger_pos, test_finger_pos, train_cursor_pos, test_cursor_pos = self._train_test_split(data, spike_matrix)\n",
    "            \n",
    "            # bin data\n",
    "            if self.bin_size > 1: \n",
    "                trainX = self._bin_data(trainX)\n",
    "                testX  = self._bin_data(testX)\n",
    "\n",
    "            # convert positions to velocity \n",
    "            trainY = self._pos2vel(train_finger_pos.numpy())\n",
    "            testY = self._pos2vel(test_finger_pos.numpy())\n",
    "            trainY_cursor = self._pos2vel(train_cursor_pos.numpy())\n",
    "            testY_cursor = self._pos2vel(test_cursor_pos.numpy())            \n",
    "            print(f'Binned into {4*self.bin_size}ms intervals')\n",
    "            \n",
    "            # normalize velocities if necessary \n",
    "            if self.normalize: \n",
    "                trainX, testX = self._normalize(trainX, testX)\n",
    "            if self.center_zero: \n",
    "                trainY, testY = self._center_zero(trainY, testY)\n",
    "                \n",
    "            # exclude channels that are too sparse \n",
    "            trainX, testX = self._exclude_channels(trainX, testX)\n",
    "            \n",
    "            self.data = trainX, testX, trainY, testY, trainY_cursor, testY_cursor\n",
    "            self.pos  = train_finger_pos, test_finger_pos, train_cursor_pos, test_cursor_pos \n",
    "            \n",
    "\n",
    "    def _get_spike_mat(self, data): \n",
    "        # extract timesteps and spike channel params\n",
    "        timesteps = torch.tensor(np.array(data['t'])).flatten()  # Ensure t is a 1D array\n",
    "        n_ch = len(data['spikes'][0])  # number of channels \n",
    "        n_sp = len(self.spike_units)   # number of spike units to use for each channel\n",
    "        \n",
    "        # initialize spike matrix with zeros \n",
    "        n = len(timesteps)\n",
    "        k = n_sp * n_ch \n",
    "        spike_matrix = np.zeros((n, k), dtype=np.float32)\n",
    "        \n",
    "        # read spike data and populate spike matrix \n",
    "        for sp_unit in self.spike_units: \n",
    "            for unit_idx, channel_ref in enumerate(data['spikes'][sp_unit]): \n",
    "                channel = data[channel_ref]  # Dereference the channel\n",
    "                for unit_ref in channel: \n",
    "                    unit_spike_times = np.array(unit_ref).flatten()  # Dereference the unit spike times and flatten\n",
    "                    spike_indices    = np.searchsorted(timesteps, unit_spike_times)\n",
    "                    spike_indices    = spike_indices[spike_indices < n]  # Ensure indices are within bounds\n",
    "                    spike_matrix[spike_indices, sp_unit*n_ch+unit_idx] = 1\n",
    "                    \n",
    "        return spike_matrix\n",
    "    \n",
    "    \n",
    "    def _train_test_split(self, data, spike_matrix):\n",
    "        finger_pos = torch.tensor(np.array(data['finger_pos']).T)  # Ensure proper shape and convert to PyTorch tensor\n",
    "        cursor_pos = torch.tensor(np.array(data['cursor_pos']).T)  # Ensure proper shape and convert to PyTorch tensor\n",
    "        # get train and test indices\n",
    "        train_idx = self.pr_dataset.ind_train\n",
    "        test_idx  = self.pr_dataset.ind_test\n",
    "        \n",
    "        train_neural = spike_matrix[train_idx]\n",
    "        test_neural  = spike_matrix[test_idx]\n",
    "        \n",
    "        train_finger_pos = finger_pos[train_idx][:,:3]  # extract z, -x, -y positions and drop spherical coords\n",
    "        test_finger_pos  = finger_pos[test_idx][:,:3]   # extract z, -x, -y positions and drop spherical coords\n",
    "        \n",
    "        train_cursor_pos = cursor_pos[train_idx]\n",
    "        test_cursor_pos  = cursor_pos[test_idx]\n",
    "        \n",
    "        return train_neural, test_neural, train_finger_pos, test_finger_pos, train_cursor_pos, test_cursor_pos\n",
    "        \n",
    "        \n",
    "    def _bin_data(self, neural): \n",
    "        n_samples, n_spikes = neural.shape\n",
    "        n_bins = int(np.ceil(n_samples / self.bin_size))\n",
    "        \n",
    "        X = np.zeros(shape=(n_bins, n_spikes), dtype=np.float32)\n",
    "        for bin, i in enumerate(range(0, n_samples, self.bin_size)): \n",
    "            X[bin] = np.mean(neural[i:i+self.bin_size], axis=0)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "    def _pos2vel(self, y): \n",
    "        n_samples, n_outputs = y.shape\n",
    "        n_bins = int(np.ceil(n_samples / self.bin_size))\n",
    "        \n",
    "        y_vel = np.zeros(shape=(n_bins, n_outputs), dtype=np.float32)\n",
    "        for bin, i in enumerate(range(0, n_samples, self.bin_size)): \n",
    "            binned_y = y[i:i+self.bin_size]\n",
    "            y_vel[bin] = binned_y[-1] - binned_y[0]\n",
    "        \n",
    "        return y_vel\n",
    "        \n",
    "    \n",
    "    def _normalize(self, trainX, testX): \n",
    "        mean = np.mean(trainX, axis=0, keepdims=True)\n",
    "        std  = np.std(trainX, axis=0, keepdims=True)\n",
    "        \n",
    "        return (trainX - mean) / std, (testX - mean) / std\n",
    "    \n",
    "    \n",
    "    def _center_zero(self, trainY, testY): \n",
    "        print(trainY.shape)\n",
    "        mean = np.mean(trainY, axis=0, keepdims=True)\n",
    "        self.finger_mean_vel = mean \n",
    "        return trainY - mean, testY - mean\n",
    "        \n",
    "    \n",
    "    def _exclude_channels(self, trainX, testX): \n",
    "        train_chs = np.sum(trainX, axis=0) > self.bad_ch_cutoff\n",
    "        test_chs  = np.sum(testX, axis=0) > self.bad_ch_cutoff\n",
    "        all_chs   = np.logical_and(train_chs, test_chs)\n",
    "        \n",
    "        return trainX[:,all_chs], testX[:,all_chs]\n",
    "    \n",
    "    \n",
    "    def __call__(self): \n",
    "        return self.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indy_20160630_01.mat\n",
      "Loaded pr dataset\n",
      "Binned into 100ms intervals\n",
      "(6899, 3)\n"
     ]
    }
   ],
   "source": [
    "dataloader = Dataloader(spike_units='all', normalize=False, center_zero=True, bin_size=25, bad_ch_cutoff=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_no_lag, testX_no_lag, trainY_no_lag, testY_no_lag, trainY_cursor_no_lag, testY_cursor_no_lag = dataloader()\n",
    "train_finger_pos, test_finger_pos, train_cursor_pos, test_cursor_pos = dataloader.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 0 # we will assume a 200ms lag\n",
    "\n",
    "# realign data to take lag into account \n",
    "trainY = trainY_no_lag; testY = testY_no_lag\n",
    "trainY_cursor = trainY_cursor_no_lag; testY_cursor = testY_cursor_no_lag\n",
    "trainX = trainX_no_lag; testX = testX_no_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggreagate all channels\n",
    "\n",
    "trainX0 = np.zeros(shape=(trainX.shape[0], 96))\n",
    "\n",
    "for i in range(1, 5): \n",
    "    trainX0 += trainX[:,96*i:96*(i+1)] \n",
    "trainX = trainX0\n",
    "\n",
    "testX0 = np.zeros(shape=(testX.shape[0], 96))\n",
    "\n",
    "for i in range(1, 5): \n",
    "    testX0 += testX[:,96*i:96*(i+1)] \n",
    "testX = testX0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4298, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee207-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
